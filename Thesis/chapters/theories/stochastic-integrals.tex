\section{Stochastic Integrals}
\label{subsection:stochastic-integrals}
\subsection{Motivation}
Suppose that we want to model the stock price where the time points are in the interval $[0,T]$. We firstly consider a discrete case where $T \in \NN$ and the intermediate time points are $t_n = n, n \in \{0,\ldots, T\}$. Let $p_0$ be the initial stock price at time $t_0$. We wish to establish the formula for the stock price at each time $t_n = n, n \in \{0,\ldots, T\}$ in a nondeterministic way, due to market uncertainty. We can instead formulate the evolution of the price over each time step. Let $\{X_n\}_{n=0}^T$ be a stochastic process where $X_n$ is the price at time $t_n=n$, we have

\begin{equation}
  \begin{cases}
    X_0= p_0 \\
    X_{n+1} - X_n = b(X_n,n) + \sigma(X_n,n)\cdot \texttt{noise}, \,\,\,n\in \{0,\cdots, T-1\},
  \end{cases}
\end{equation}

where $b,\sigma:\RR\times\NN\to\RR$. We select the standard normal random variable for this noise
$$\texttt{noise}:= Z_{n} = W_{n+1}-W_n \sim \N(0,1),$$

by which we hypothesized that if the noise acts more complicatedly, then we can adjust $\sigma(X_n,n)$ to be $\hat{\sigma}(X_n,n)$ such that
$$\sigma(X_n,n)\cdot \texttt{noise} = \hat{\sigma}(X_n,n)(W_{n+1}-W_n) \text{ in } L^2.$$

Our reasonable model is
\begin{equation}
  \label{example:stock-price}
  \begin{cases}
    X_0= p_0 \\
    X_{n+1} - X_n = b(X_n,n) + \sigma(X_n,n)\cdot Z_{n}, \,\,\,n\in \{0,\cdots, T-1\},
  \end{cases}
\end{equation}

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{img/stock-price-sampling.png}
  \vspace{0.25cm}
  \caption[Stock price simulation]{A simulation of stock price evolution by $X_{n+1} - X_n = rX_n + \alpha X_n\cdot Z_{n}$, where $p_0=50, r = 0.005
      \alpha = 0.1$}
\end{figure}

By using time discretization, when it requires to estimate the price at non-integer time, say $t=n+\dfrac{1}{2}$, we need to completely rebuild our model. But intuitively, there must be some change from time $n$ to $n+\dfrac{1}{2}$, then to $n+1$. It√¥ successfully smoothed the model by Riemann approximation in order to predict the price at any time point in $t\in[0,T]$. Let $P^n[0,T]:=\{0=t^n_0<t^n_1<\ldots<t^n_{m_n}=t\}$ be partitions on $[0,T]$ such that $|P^n|\to 0$ as $n\to\infty$ and write
\begin{align*}
  X(t^n_{k+1})- X(t^n_{k})
   & = b(X(t^n_{k}),t^n_{k})(t^n_{k+1} - t^n_{k+1})+ \sigma(X(t^n_{k}),t^n_{k})(W(t^n_{k+1})-W(t^n_{k})) \\
   & = b(X(t^n_{k}),t^n_{k})\Delta t^n_{k} + \sigma(X(t^n_{k}),t^n_{k})\Delta W(t^n_{k}).
\end{align*}

We have
$$X(t)=X(0)+\sum\limits_{k=0}^{m_n-1}  b(X_k^n,t_k^n)\Delta t^n_{k} + \sum\limits_{k=0}^{m_n-1} \sigma(X(t^n_{k}),t^n_{k})\Delta W(t^n_{k}).$$

Let $(\Omega, \F)$ be a measurable space for the stochastic processes $\{X_t\}_{0\le t\le T}$ and $\{W_t\}_{0\le t\le T}$. Set
$$\hat{b}(\omega, t) :=  b(X_t(\omega),t) \text{ and } \hat{\sigma}(\omega, t) := \sigma(X_t(\omega),t), \forall \omega\in\Omega, t\in[0,T].$$

Then
\begin{equation}
  \label{equation:stochastic-sum}
  X(t,\omega)=X(0,\omega)+\sum\limits_{k=0}^{m_n-1}  \hat{b}(\omega, t_k^n)\Delta t^n_{k} + \sum\limits_{k=0}^{m_n-1} \hat{\sigma}(\omega, t_k^n)\Delta W(t^n_{k}), \forall \omega\in\Omega.
\end{equation}

For each $\omega\in\Omega$, the first sum $\hat{b}(\omega, t_k^n)\Delta t^n_{k}$ converges to the integral $\int\limits_0^T \hat{b}(\omega, t)\d t$, under an assumption that $t\mapsto \hat{b}(\omega, t)$ is Riemann integrable. Therefore, we will try to find the limit of the second sum under appropriate conditions. Note that when we construct such sum, we implicitly assume that $\hat{b}(\omega, t)$ does not depend on $W_s$, for $s\ge t$ i.e. $\hat{b}(\omega, t)$ only depends on the history of the Brownian motion $\{W_t\}$ up to time $t$, not the future of $\{W_t\}$. Let us give a definition for such process.

\begin{definition}[Non-anticipating process]
  Let $W:=\{W_t\}_{t\in[0,T]}$ be a Wiener process, and $\{\W_t\}_{t\in[0,T]}$ be the right-continuous completion of the natural filtration of $W$. Let $\G$ be a $\sigma$-algebra independent of ${\W_t}_{t\in[0,T]}$. Then the non-anticipating filtrations are the ones of the form
  $$\{\sigma(\W_t \cup G)\}_{t\in[0,T]}.$$
  A stochastic process $\{X_t\}_{t\in[0,T]}$ is non-anticipating if it is adapted to some non-anticipating filtration.
\end{definition}

Further conditions are added to guarantee the convergence of $hat{b}(\omega, t_k^n)$, which will be clarified later. In such case, we adapt the usual integration notation as
\begin{equation}
  \sum\limits_{k=0}^{m_n-1} \hat{\sigma}(\omega, t_k^n)\Delta W(t^n_{k}) \xrightarrow{n\to\infty} \int\limits_0^t \hat{\sigma}(\omega, t_k^n)\d W_s = \int\limits_0^t \sigma(X_s,s)\d W_s.
\end{equation}

\subsection{An Example on the Wiener Process}

In Equation \ref{equation:stochastic-sum}. We have selected in initial points $(t_k^n)_{k=1}^{m_n-1}$ in the partition for the limit of the approximations. Unlike the Riemann integral, each point $\tau_k^n = (1-\lambda) t_k^n + \lambda t_{k+1}^n$, where $\lambda\in[0,1]$, for $k=1,\ldots, m_n-1$ results in different a value of the limit
$$\sum\limits_{k=0}^{m_n-1} \hat{\sigma}(\tau_k^n, \omega)\Delta W(\tau^n_{k}).$$

Let us take a ubiquitous example to emphasize this fact.

\begin{example}
  \label{example:wdw}
  Let $P^n[0,T]:=\{0=t^n_0<t^n_1<\ldots<t^n_{m_n}=T\}$ be partitions on $[0,T]$ such that $|P^n|\to0$ when $n\to\infty$ and $\{W_t\}_{t\in[0,T]}$ be real-valued Wiener process. Calculate the limit
  \begin{equation}
    \lim\limits_{n\to\infty}\sum\limits_{k=0}^{m_n-1} W(\tau_k^n)(W(t^n_{k+1})-W(t^n_{k})),
  \end{equation}
  where
  $\tau_k^n = (1-\lambda) t_k^n + \lambda t_{k+1}^n$ for some $\lambda\in[0,1]$.
\end{example}

\begin{solution}
  We have
  \begin{align*}
    R_n
     & = \sum\limits_{k=0}^{m_n-1} W(\tau_k^n)(W(t^n_{k+1})-W({t^n_{k}}))                                                                      \\
     & =
    \sum\limits_{k=0}^{m_n-1}\left[ W(\tau_k^n) - W(t_k^n) + W(t_k^n)\right]\left[W(t^n_{k+1}) - W(\tau_k^n) + W(\tau_k^n) - W(t^n_{k})\right] \\
     & = \underbrace{\sum\limits_{k=0}^{m_n-1}(W(\tau_k^n) - W(t_k^n))(W(t^n_{k+1}) - W(\tau_k^n)) }_{A_n}
    \\
     & \,\,\,\, + \underbrace{\sum\limits_{k=0}^{m_n-1}(W(\tau_k^n) - W(t_k^n))^2  }_{B_n}
    \\
     & \,\,\,\, + \underbrace{ \sum\limits_{k=0}^{m_n-1}(W(t_k^n))(W(t^n_{k+1}) - W(t_k^n))}_{C_n}
  \end{align*}

  Now we study the terms $A_n$, $B_n$ and $C_n$. Let us show that $A_n\to 0$ in $L^2(\Omega)$. Indeed, we have
  \begin{align*}
    \EE[A_n^2]
     & = \EE\left[\left(\sum\limits_{k=0}^{m_n-1}(W(\tau_k^n) - W(t_k^n))(W(t^n_{k+1}) - W(\tau_k^n))\right)^2\right]        \\
     & = \EE\left[\sum\limits_{k=0}^{m_n-1}[(W(\tau_k^n) - W(t_k^n))(W(t^n_{k+1}) - W(\tau_k^n))]^2\right]                   \\
     & \,\,\,\, + \EE\left[\sum\limits_{\substack{k,\ell=0                                                                   \\ k\ne \ell}}^{m_n-1}(W(\tau_k^n) - W(t_k^n))(W(t^n_{k+1}) - W(\tau_k^n))(W(\tau_\ell^n) - W(t_\ell^n))(W(t^n_{\ell+1}) - W(\tau_\ell^n))\right]\\
     & = \sum\limits_{k=0}^{m_n-1}\EE\left[(W(\tau_k^n) - W(t_k^n))^2(W(t^n_{k+1}) - W(\tau_k^n))^2\right]                   \\
     & \,\,\,\, + \sum\limits_{\substack{k,\ell=0                                                                            \\ k\ne \ell}}^{m_n-1}\EE\left[(W(\tau_k^n) - W(t_k^n))(W(t^n_{k+1}) - W(\tau_k^n))(W(\tau_\ell^n) - W(t_\ell^n))(W(t^n_{\ell+1}) - W(\tau_\ell^n))\right]\\
     & = \sum\limits_{k=0}^{m_n-1}\EE\left[(W(\tau_k^n) - W(t_k^n))^2(W(t^n_{k+1}) - W(\tau_k^n))^2\right]                   \\
     & \,\,\,\, + \sum\limits_{\substack{k,\ell=0                                                                            \\ k\ne \ell}}^{m_n-1}\EE\left[(W(\tau_k^n) - W(t_k^n))(W(t^n_{k+1}) - W(\tau_k^n))(W(\tau_\ell^n) - W(t_\ell^n))(W(t^n_{\ell+1}) - W(\tau_\ell^n))\right]\\
     & = \sum\limits_{k=0}^{m_n-1}\EE\left[(W(\tau_k^n) - W(t_k^n))^2\right]  \EE\left[(W(t^n_{k+1}) - W(\tau_k^n))^2\right] \\
     & \,\,\,\, + \sum\limits_{\substack{k,\ell=0                                                                            \\ k\ne \ell}}^{m_n-1}\underbrace{\EE\left[W(\tau_k^n) - W(t_k^n)\right]}_{=0}\EE\left[(W(t^n_{k+1}) - W(\tau_k^n))(W(\tau_\ell^n) - W(t_\ell^n))(W(t^n_{\ell+1}) - W(\tau_\ell^n))\right]\\
     & = \sum\limits_{k=0}^{m_n-1}\EE\left[(W(\tau_k^n) - W(t_k^n))^2\right]  \EE\left[(W(t^n_{k+1}) - W(\tau_k^n))^2\right] \\
     & = \sum\limits_{k=0}^{m_n-1}(1-\lambda)(t_{k+1}^n - t_k)\lambda(t_{k+1}^n - t_k^n)                                     \\
     & \le \lambda(1-\lambda) \sum\limits_{k=0}^{m_n-1}(t_{k+1}^n - t_k)|P^n|                                                \\
     & = \lambda(1-\lambda)T|P^n| \xrightarrow{n\to\infty} 0.
  \end{align*}

  Next, we will show that $B_n\to \lambda T$ in $L^2(\Omega)$.
  \begin{align*}
    \EE[(B_n-\lambda T)^2]
     & = \EE[B_n^2+\lambda^2 T^2 - 2\lambda T B_n]                                                                                                                                            \\
     & = \lambda^2 T^2 - 2\lambda T \EE[ B_n] + \EE[B_n^2]                                                                                                                                    \\
     & = \lambda^2 T^2 - 2\lambda T \sum\limits_{k=0}^{m_n-1} \EE\left[(W(\tau_k^n) - W(t_k^n))^2\right] + \EE\left[\left(\sum\limits_{k=0}^{m_n-1}(W(\tau_k^n) - W(t_k^n))^2\right)^2\right] \\
     & = \lambda^2 T^2 - 2\lambda T \sum\limits_{k=0}^{m_n-1} \lambda (t_{k+1}^n-t_k^n)                                                                                                       \\
     & \,\,\,\, + \sum\limits_{k=0}^{m_n-1}\EE\left[(W(\tau_k^n) - W(t_k^n))^4\right] + \sum\limits_{\substack{k,\ell=0                                                                       \\ k\ne \ell}}^{m_n-1} \EE\left[(W(\tau_k^n) - W(t_k^n))^2(W(\tau_\ell^n) - W(t_\ell^n))^2\right]                                                                                                    \\
     & = -\lambda^2 T^2 + 3\lambda^2\sum\limits_{k=0}^{m_n-1}(t_{k+1}-t_k)^2                                                                                                                  \\
     & \,\,\,\,+ \sum\limits_{\substack{k,\ell=0                                                                                                                                              \\ k\ne \ell}}^{m_n-1} \EE\left[(W(\tau_k^n) - W(t_k^n))^2\right] \EE\left[(W(\tau_\ell^n) - W(t_\ell^n))^2\right] \\
     & = -\lambda^2 T^2 + 3\lambda^2\sum\limits_{k=0}^{m_n-1}(t_{k+1}^n-t_k^n)^2 + \lambda^2\sum\limits_{\substack{k,\ell=0                                                                   \\ k\ne \ell}}^{m_n-1} (t_{k+1}^n-t_k^n)(t_{\ell+1}^n-t_\ell^n) \\
     & = -\lambda^2 T^2 + 2\lambda^2\sum\limits_{k=0}^{m_n-1}(t_{k+1}^n-t_k^n)^2 + \lambda^2\sum\limits_{\substack{k,\ell=0}}^{m_n-1} (t_{k+1}^n-t_k^n)(t_{\ell+1}^n-t_\ell^n)                \\
     & = -\lambda^2 T^2 + 2\lambda^2\sum\limits_{k=0}^{m_n-1}(t_{k+1}^n-t_k^n)^2 + \lambda^2\sum\limits_{\substack{k,\ell=0}}^{m_n-1} (t_{k+1}^n-t_k^n)(t_{\ell+1}^n-t_\ell^n)                \\
     & = 2\lambda^2\sum\limits_{k=0}^{m_n-1}(t_{k+1}^n-t_k^n)^2                                                                                                                               \\
     & \le 2\lambda^2\sum\limits_{k=0}^{m_n-1}|P^n|(t_{k+1}^n-t_k^n)                                                                                                                          \\
     & = 2\lambda^2|P^n|T \xrightarrow{n\to\infty} 0.
  \end{align*}

  The last expression $C_n$ is exactly the case when we choose the initial points.

  Since
  \begin{align*}
    W(t_k^n)(W(t^n_{k+1})-W(t^n_k))
  \end{align*}
  We have
  \begin{align*}
    C_n
     & =\sum\limits_{k=0}^{m_n-1}W(t^n_k)(W(t^n_{k+1})-W(t^n_k))                          \\
     & = \dfrac{1}{2}\left[W^2(t^n_{k+1})-W^2(t^n_k)-(W(t^n_{k+1})-W(^n_k))^2\right]
     & =\dfrac{W^2(T)}{2} -\dfrac{1}{2}\sum\limits_{k=0}^{m_n-1}(W(t^n_{k+1})-W(t^n_k))^2 \\
     & \xrightarrow{n\to\infty} \dfrac{W^2(T)}{2} - \dfrac{T}{2}
  \end{align*}
  Thus,
  $$R_n \xrightarrow{n\to\infty} \dfrac{W^2(T)}{2} + \left(\lambda-\dfrac{1}{2}\right)T.$$
\end{solution}

Two following choices of $\lambda$ are popular in particular
\begin{enumerate}
  \item If $\lambda = 0$, the limit $\lim\limits_{n\to\infty}R_n$ is called an It√¥ integral, denoted by the conventional integral notation $\int\limits_0^T W\d W$.
  \item If $\lambda = \dfrac{1}{2}$, it is called a Stratonovich integral, denoted by $\int\limits_0^T W\circ\d W$.
\end{enumerate}

In the scope of our project, we concern about the It√¥ Integral for later applications on stochastic modeling.

\subsection{Construction of the It√¥ Integral}

Recall that we have adapted the bootstrapping process to construct the Lebesgue integral. In the construction of the It√¥ Integral, similar strategy is applicable. We define It√¥ integral for the class of elementary functions, then generalize the result for a function $\hat{\sigma}(\omega, t)$ satisfying some additional conditions. Now we make consistent definitions for such class of functions.

\begin{definition}[Progressively measurable process]
  Let $\{X_t\}_{t\in[0,T]}$ be a stochastic process adapted to a filtration $\{\F_t\}_{t\in[0,T]}$. Then $\{X_t\}_{t\in[0,T]}$ is said to be progressively measurable (progressive) if $X_t$ is $\F_t\otimes\B([0,t])$, for any $t\in[0,T]$.
\end{definition}

\begin{definition}[Elementary process]
  A progressive, non-anticipating process $\{X_t\}_{t\in[0,T]}$ is elementary if there exists a partition
  $$P[0,T]:=\{0=t_0<t_1<\ldots<t^N=T\}$$ such that
  $$X(t) = X(t_n) \text{ if } t \in [t_n, t_{n+1}), \text{ for } j=0,\ldots,N-1.$$
\end{definition}
\begin{remark}
  For an elementary process, the map $t\mapsto X(t,\omega)$ is a step function. We can write an elementary compactly as
  $$X(t) = \sum\limits_{n=0}^{N-1} X(t_n)\mathbf{1}_{[t_n, t_{n+1})}(t),$$
  where $\mathbf{1}$ is the indicator function.
\end{remark}

Denote by $\V := \V[0,T]$ the class of $\L^2$ stochastic processes that are progressively measurable and non-anticipating. We will prove that any process in $\V$ can be approximated by a sequence of elementary process. Throughout the proofs, we will see reasons why a process must be in class $\V$ so that it is It√¥ integrable.

\begin{definition}
  Let $\{X_t\}_{t\in[0,T]}$ be an elementary process defined on a partition $P[0,T]:=\{0=t_0<t_1<\ldots<t_N=T\}$. The It√¥ integral of $\{X_t\}_{t\in[0,T]}$ on $[0,t]$ is
  $$\int\limits_0^t X(t)\d W = \sum\limits_{j=0}^{n-1} X(t_j)(W_{j+1}-W_{j}).$$
  If $\int\limits_0^t X(t)\d W < \infty$, the process $\{X_t\}_{t\in[0,T]}$ is said to be It√¥-integrable.
\end{definition}

\begin{lemma}[Approximation of a bounded, continuous process by elementary processes]
  \label{lemma:approximation-1}
  Let $X:=\{X_t\}_{t\in[0,\infty]}$ be a stochastic process that is continuous, progressive, non-anticipating and bounded on $[0, T]$. Then there exists a sequence $(X_n)_{n\in\NN} := (\{X_n(t)\}_{t\in[0,\infty]})_{n\in\NN}$ of bounded, It√¥-integrable elementary processes such that
  $$X_n \xrightarrow{n\to\infty} X \text{ in } \L^2.$$
\end{lemma}

\begin{proof}
  Set
  \begin{equation}
    X_n(t) := \sum\limits_{i=1}^\infty X\left(i/2^n\right) \1_{[i/2^n, (i+1)/2^n)}(t).
  \end{equation}
  This is clearly elementary, bounded and square-integrable on $[0, T]$. For any fix $\omega$, since $X(t,\omega)$ is continuous, we have
  $$\lim\limits_{n\to\infty}\int\limits_0^T (X(t,\omega) - X_n(t,\omega))^2 \d t = 0.$$

  By dominated convergence,

  \begin{align*}
    \lim\limits_{n\to\infty}\|X(t) - X_n(t)\|_{\L^2}
     & = \lim\limits_{n\to\infty}\int_\Omega\int\limits_0^T (X(t,\omega) - X_n(t,\omega))^2 \d t \d P \\
     & = \int_\Omega\int\limits_0^T \lim\limits_{n\to\infty}(X(t,\omega) - X_n(t,\omega))^2 \d t \d P \\
     & = 0.
  \end{align*}
\end{proof}

\begin{lemma}[Approximation of a bounded process by bounded, continuous processes]
  \label{lemma:approximation-2}
  Let $X$ be a stochastic process that is progressive, non-anticipating, bounded on $[0, T]$. Then there exists a sequence $(X_n)_{n\in\NN}$ of continuous, progressive, non-anticipating and bounded on $[0, T]$ processes such that
  $$X_n \xrightarrow{n\to\infty} X \text{ in } \L^2.$$
\end{lemma}
\begin{proof}
  Let $M$ be the bound on the norm of X. For each $n$, pick a probability density $f_n(t):\RR\to\RR$ support is the interval $(-1/n, 0)$. Set
  $$X_n(t) := \int\limits_0^t f_n(s-t) X(s) \d s.$$
  Then $X_n$ is continuous, bounded, progressively measurable, and non-anticipating. Therefore, for any fix $\omega$, we
  $$\int\limits_0^T (X(t,\omega) - X_n(t,\omega))^2 \d t \xrightarrow{n\to\infty} 0.$$
  Convergence in $\L^2$ follows from dominated convergence.
\end{proof}


\begin{lemma}[Approximation of a $\V$ process by bounded processes]
  \label{lemma:approximation-3}
  Let $X$ be an $\L^2$ stochastic process that is progressive, non-anticipating. Then there exists a sequence $(X_n)_{n\in\NN}$ of progressive, non-anticipating and bounded on $[0, T]$ processes such that
  $$X_n \xrightarrow{n\to\infty} X \text{ in } \L^2.$$
\end{lemma}
\begin{proof}
  Set $X_n(t) = (-n \vee X(t)) \wedge n$. Then $X_n$ is bounded. The result follows from dominated convergence.
\end{proof}

\begin{theorem}[Approximation of a $\V$ process by elementary processes]
  Let $X$ be an $\L^2$ stochastic process that is progressive, non-anticipating. Then there exists a sequence $(X_n)_{n\in\NN}$ of bounded, It√¥-integrable elementary processes such that
  $$X_n \xrightarrow{n\to\infty} X \text{ in } \L^2.$$
\end{theorem}
\begin{proof}
  The result follows by Lemmas \ref{lemma:approximation-1}, \ref{lemma:approximation-2} and \ref{lemma:approximation-3}.
\end{proof}

\begin{lemma}[It√¥ isometry for elementary processes]
  \label{lemma:ito-isometry-for-elementary-process}
  Let $\{X_t\}_{t\in[0,T]}$ be a elementary process. Then
  $$\EE\left[\left(\int\limits_0^T X\d W\right)^2\right] = \EE\left[\int\limits_0^T X^2\d t\right] = \|X\|_{\L^2}.$$
\end{lemma}


\begin{proof}
  Let $R_n=\sum\limits_{k=0}^{m_n-1} X(t_k^n)\Delta W(t^n_{k})$. We have
  \begin{align*}
    \EE\left[R_n^2\right]
     & = \EE\left[\sum\limits_{k,\ell = 0}^{m_n-1}X(t_k^n)X(t_\ell^n)\Delta W(t^n_{k}) \Delta W(t^n_{\ell})\right] \\
     & = \sum\limits_{k,\ell = 0}^{m_n-1}\EE\left[X(t_k^n)X(t_\ell^n)\Delta W(t^n_{k}) \Delta W(t^n_{\ell})\right]
  \end{align*}
  If $k \ne \ell$, then $\Delta W(t^n_{k})$ and $\Delta W(t^n_{\ell})$ are independent. Hence,
  \begin{align*}
    \EE\left[X(t_k^n)X(t_\ell^n)\Delta W(t^n_{k}) \Delta W(t^n_{\ell})\right]
     & = \EE\left[X(t_k^n)X(t_\ell^n)\Delta W(t^n_{k})\right]\EE\left[\Delta W(t^n_{\ell})\right] \\
     & =0.
  \end{align*}

  If $k = \ell$, then
  \begin{align*}
    \EE\left[X(t_k^n)X(t_\ell^n)\Delta W(t^n_{k}) \Delta W(t^n_{\ell})\right]
     & =\EE\left[X^2(t_k^n)\right] \EE\left[\Delta W(t^n_{k})^2\right] \\
     & = \EE\left[X^2(t_k^n)\right] (t_{k+1}^n-t_k^n).
  \end{align*}
  Therefore,
  \begin{align*}
    \EE\left[R_n^2\right]
     & =\sum\limits_{k}^{m_n-1} \EE\left[X^2(t_k^n)\right] (t_{k+1}^n-t_k^n)
  \end{align*}
  Taking the limits when $n\to\infty$, we complete the proof.
\end{proof}

\begin{theorem}[It√¥ integrals of Approximating Elementary Processes Converge]
  Let $(X_n)_{n\in\NN}$ be a sequence of $\V$ elementary processes converging to $X$ in $\L^2$. Then the sequence
  $$R_n(X) = \lim\limits_{n\to\infty}\int\limits_0^T X_n\d W$$
  converges in $L^2$. Moreover, this limit is the same for any such approximating sequence $(X_n)$.
\end{theorem}

\begin{proof}
  Since $\L^2$ is Banach and $(X_n)$ converges, $(X_n)$ is Cauchy. Hence, for any $\epsilon>0$, there exists $N\in\NN$ such that
  $$\|X_{n+N} - X_{N}\|\le\epsilon.$$
  Since $X_{n+N}$ and $X_{N}$ are elementary, so is their difference. By Lemma \ref{lemma:ito-isometry-for-elementary-process},
  $$\EE\left[\left(\int\limits_a^b (X_{n+N}-X_n)\d W \right)^2\right] < \epsilon^2.$$
  Therefore, $(R_n)$ is Cauchy in $L^2$. Since $L^2$ is Banach, $(R_n)$ has a limit. If there exists another approximating elementary sequence $(Y_n)$, then their It√¥ integral sequence $(S_n)$ is also Cauchy. On the other hand, for any $\epsilon >0$, there exist $m,n\in\NN$ such that
  $$\|X_m-Y_n\| \le \|X_m-X\| + \|Y_n - X\|\le \epsilon.$$
  Hence, the limits of their It√¥ integral must be equal.
\end{proof}

\begin{definition}
  Let $X\in \V$. The It√¥ integral of $X$ is defined by
  \begin{equation}
    \int\limits_0^T X\d W =  \lim\limits_{n\to\infty}\int\limits_0^T X_n\d W,
  \end{equation}
  where $(\{X_n(t)\}_{t\in[0,T]})_{n\in\NN}$ is a sequence of elementary functions such that
  $$X_n \xrightarrow{n\to\infty} X \text{ in } \L^2.$$
\end{definition}

\begin{theorem}[Properties of It√¥ integral]
  \label{theorem:prop}
  For all constants $a,b\in\RR$ and $\sigma,H\in\mathcal{V}(S,T)$, we have
  \begin{enumerate}
    \item Linearity: $\int\limits_{S}^T(aG+bH)\d W = a\int\limits_{S}^TG\d W+b\int\limits_{S}^TH\d W$.
    \item $\EE\left[\int\limits_{S}^TG\d W\right]=0$.
    \item It√¥ isometry: $\EE\left[\left(\int\limits_{S}^TG\d W\right)^2\right]=\EE\left[\int\limits_{S}^TG^2\d t\right]$.
    \item $\EE\left[\int\limits_{S}^TG\d W\int\limits_{S}^TH\d W\right]=\EE\left[\int\limits_{S}^TGH\d W\right]$.
  \end{enumerate}
\end{theorem}

With the linearity property, we are able to define multidimensional It√¥ integral.

\begin{definition}
  Let $\sigma=(G_{ij})$, where $G_{ij}, 1\le i\le n, 1\le j\le m$ be real-valued stochastic processes. Also, $W=(W_j)$, where $W_j,1\le j\le m$ be real-valued Wiener process. Then for some $T\ge S\ge 0$
  \begin{equation}
    \int\limits_S^T \sigma\d W=\int\limits_S^T\begin{pmatrix}
      G_{11} & \cdots & G_{1m} \\
      \vdots & \ddots & \vdots \\
      G_{n1} & \cdots & G_{nm}
    \end{pmatrix}\begin{pmatrix}
      \d W_1 \\\vdots\\\d W_m
    \end{pmatrix}
  \end{equation}
  is an $n$-dimensional vector whose $i$th column is the sum of $m$ one-dimensional integrals
  $$\sum\limits_{j=1}^m \int\limits_S^T G_{ij}\d W_k.$$
\end{definition}

\subsection{It√¥'s Formulas}
Similarly to ordinary differential equations, there are the chain rule and the product rule in SDE calculus.
\begin{theorem}[One-dimensional It√¥'s chain rule]
  Suppose that $X_t$ has the stochastic differential
  $$\d X = u\d t + v\d W_t.$$
  Let $g\in C^2(\RR\times[0,\infty))$. Then
  $$Y_t=g(X_t,t)$$
  is given by
  \begin{equation}
    \label{equation:1dchainrule}
    \d Y_t=\dfrac{\partial g}{\partial t}(X_t,t)\d t+\dfrac{\partial g}{\partial x}(X_t,t)\d X_t+\dfrac{1}{2}\dfrac{\partial^2 g}{\partial x^2}(X_t,t)\cdot(\d X_t)^2,
  \end{equation}
  where $(\d X_t)^2=(\d X_t)\cdot(\d X_t)$ is computed according to the rules
  $$\d t\cdot\d t = \d t\cdot\d W_t = \d W_t\cdot\d t = 0, \d W_t\cdot\d W_t=\d t.$$
\end{theorem}

\begin{theorem}[One-dimensional It√¥'s product rule]
  Suppose $\{X_1(t)\}_{[0,T]}$ and $\{X_2(t)\}_{[0,T]}$ satisfy the following SDEs
  $$\begin{cases}
      \d X_1 = b_1\d t+\sigma_1\d W \\
      \d X_2 = b_2\d t+\sigma_2\d W
    \end{cases}(0\le t\le T).$$
  Then
  \begin{equation}
    \label{theorem:ito-product}
    \d (X_1X_2)=X_2\d X_1+X_1\d X_2+\sigma_1\sigma_2\d t
  \end{equation}
\end{theorem}

% \begin{theorem}[Multidimensional It√¥'s chain rule]
% \label{equation:mdchainrule}
% \end{theorem}


\subsection{Properties of the It√¥ Process}

\begin{definition}
  Let $b$ be a non-anticipating measurable process, $\sigma$ be It√¥-integrable, and $X_0$ be an $L^2$ random variable independent of $W$. Then the integral
  $$X_t := X_0 + \int_0^t b(s) \d s + \int_0^t \sigma(s) dW$$
  is called an It√¥ process.
\end{definition}

% \begin{theorem}
%   \label{every-ito-process-is-non-anticipating}
%   Every It√¥ process is non-anticipating.
% \end{theorem}